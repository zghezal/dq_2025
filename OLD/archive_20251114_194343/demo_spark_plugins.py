"""
DÃ©monstration des plugins DQ Spark-natifs.

Ce script montre comment utiliser les plugins avec SparkDQContext
pour exÃ©cuter des contrÃ´les qualitÃ© sur des datasets Spark.
"""

import sys
import os
import logging

# Configurer le logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Ajouter le dossier src au path (si nÃ©cessaire)
sys.path.insert(0, os.path.abspath('.'))

# Imports
from src.context.spark_context import SparkDQContext
from src.plugins.metrics.missing_rate import MissingRate
from src.plugins.metrics.aggregation_by_column import AggregationByColumn


def demo_basic():
    """DÃ©monstration basique avec les mÃ©triques Spark."""
    
    print("\n" + "=" * 70)
    print("DÃ‰MONSTRATION 1 : MÃ©triques basiques avec Spark")
    print("=" * 70)
    
    # 1. CrÃ©er le catalogue
    catalog = {
        "customers": "data/customers.parquet",
        "sales": "data/sales.parquet",
        "products": "data/products.parquet"
    }
    
    # 2. Initialiser le context Spark
    print("\nğŸš€ Initialisation du SparkDQContext...")
    context = SparkDQContext(catalog=catalog)
    
    # 3. Test Missing Rate sur customers.email
    print("\n" + "-" * 70)
    print("ğŸ“Š Test 1 : Missing Rate sur customers.email")
    print("-" * 70)
    
    plugin_missing = MissingRate()
    result = plugin_missing.run(
        context,
        dataset="customers",
        column="email"
    )
    
    print(f"âœ… RÃ©sultat: {result.message}")
    print(f"   Taux de missing: {result.value:.2%}")
    print(f"   Meta: {result.meta}")
    
    # 4. Test Missing Rate sur sales.amount
    print("\n" + "-" * 70)
    print("ğŸ“Š Test 2 : Missing Rate sur sales.amount")
    print("-" * 70)
    
    result2 = plugin_missing.run(
        context,
        dataset="sales",
        column="amount"
    )
    
    print(f"âœ… RÃ©sultat: {result2.message}")
    print(f"   Taux de missing: {result2.value:.2%}")
    
    # 5. Test Aggregation sur sales par region
    print("\n" + "-" * 70)
    print("ğŸ“Š Test 3 : Aggregation sales par region")
    print("-" * 70)
    
    plugin_agg = AggregationByColumn()
    result3 = plugin_agg.run(
        context,
        dataset="sales",
        group_by="region",
        target="amount"
    )
    
    print(f"âœ… RÃ©sultat: {result3.message}")
    print(f"   Nombre de groupes: {len(result3.dataframe)}")
    print("\n   RÃ©sultats agrÃ©gÃ©s:")
    print(result3.dataframe.to_string(index=False))
    
    # 6. Nettoyer
    print("\nğŸ§¹ Nettoyage...")
    context.stop()
    print("âœ… Context fermÃ©")


def demo_with_pandas():
    """DÃ©monstration avec conversion automatique Pandas â†’ Spark."""
    
    print("\n" + "=" * 70)
    print("DÃ‰MONSTRATION 2 : Conversion automatique Pandas â†’ Spark")
    print("=" * 70)
    
    import pandas as pd
    
    # CrÃ©er un petit DataFrame Pandas pour test
    df_test = pd.DataFrame({
        'id': [1, 2, 3, 4, 5],
        'value': [10.0, None, 30.0, None, 50.0],
        'category': ['A', 'A', 'B', 'B', 'C']
    })
    
    print("\nğŸ“Š DataFrame Pandas de test:")
    print(df_test)
    
    # CrÃ©er un context avec ce DataFrame
    context = SparkDQContext(catalog={
        "test_data": df_test  # Pandas â†’ Spark automatiquement
    })
    
    # Test Missing Rate
    print("\n" + "-" * 70)
    print("ğŸ“Š Missing Rate sur 'value'")
    print("-" * 70)
    
    plugin = MissingRate()
    result = plugin.run(context, dataset="test_data", column="value")
    
    print(f"âœ… RÃ©sultat: {result.message}")
    print(f"   Expected: 40% (2 nulls sur 5 valeurs)")
    print(f"   Actual: {result.value:.2%}")
    
    # Test Aggregation
    print("\n" + "-" * 70)
    print("ğŸ“Š Aggregation par 'category'")
    print("-" * 70)
    
    plugin_agg = AggregationByColumn()
    result2 = plugin_agg.run(
        context,
        dataset="test_data",
        group_by="category",
        target="value"
    )
    
    print(f"âœ… RÃ©sultat: {result2.message}")
    print("\n   RÃ©sultats agrÃ©gÃ©s:")
    print(result2.dataframe.to_string(index=False))
    
    context.stop()


def demo_cache():
    """DÃ©monstration du systÃ¨me de cache Spark."""
    
    print("\n" + "=" * 70)
    print("DÃ‰MONSTRATION 3 : Cache Spark")
    print("=" * 70)
    
    catalog = {
        "customers": "data/customers.parquet"
    }
    
    context = SparkDQContext(catalog=catalog)
    
    # Premier chargement (sans cache)
    print("\nğŸ“‚ Premier chargement (lecture depuis fichier)...")
    df1 = context.load("customers")
    print(f"   Lignes: {df1.count()}")
    
    # DeuxiÃ¨me chargement (depuis cache)
    print("\nğŸ“¦ DeuxiÃ¨me chargement (depuis cache)...")
    df2 = context.load("customers")
    print(f"   Lignes: {df2.count()}")
    print("   âš¡ Chargement instantanÃ© depuis le cache!")
    
    # VÃ©rifier qu'ils pointent vers le mÃªme objet
    print(f"\nğŸ” MÃªme objet? {df1 is df2}")
    
    context.stop()


def demo_output_schema():
    """DÃ©monstration du output_schema."""
    
    print("\n" + "=" * 70)
    print("DÃ‰MONSTRATION 4 : Output Schema")
    print("=" * 70)
    
    # Afficher le schÃ©ma prÃ©dit pour aggregation
    params = {
        "dataset": "sales",
        "group_by": "region",
        "target": "amount"
    }
    
    schema = AggregationByColumn.output_schema(params)
    
    print("\nğŸ“‹ SchÃ©ma prÃ©dit pour AggregationByColumn:")
    print(f"   Dataset: {params['dataset']}")
    print(f"   Group by: {params['group_by']}")
    print(f"   Target: {params['target']}")
    print("\n   Colonnes produites:")
    
    for col in schema.columns:
        print(f"   - {col.name:20s} ({col.dtype:8s}) - {col.description}")
    
    # ExÃ©cuter et vÃ©rifier
    print("\nğŸ”„ ExÃ©cution de la mÃ©trique...")
    
    catalog = {"sales": "data/sales.parquet"}
    context = SparkDQContext(catalog=catalog)
    
    plugin = AggregationByColumn()
    result = plugin.run(context, **params)
    
    print(f"âœ… {result.message}")
    print(f"\n   Colonnes rÃ©elles: {list(result.dataframe.columns)}")
    print("   âœ… SchÃ©ma validÃ© automatiquement!")
    
    context.stop()


def main():
    """Point d'entrÃ©e principal."""
    
    print("\n" + "=" * 70)
    print("ğŸš€ DÃ‰MONSTRATION DES PLUGINS DQ SPARK-NATIFS")
    print("=" * 70)
    
    # VÃ©rifier que les donnÃ©es existent
    if not os.path.exists("data/customers.parquet"):
        print("\nâŒ ERREUR: Fichiers de donnÃ©es introuvables!")
        print("   ExÃ©cutez d'abord: python generate_test_data.py")
        return
    
    try:
        # Lancer les dÃ©mos
        demo_basic()
        demo_with_pandas()
        demo_cache()
        demo_output_schema()
        
        # RÃ©sumÃ©
        print("\n" + "=" * 70)
        print("âœ… TOUTES LES DÃ‰MONSTRATIONS TERMINÃ‰ES")
        print("=" * 70)
        print("\nğŸ“ RÃ©sumÃ©:")
        print("   âœ… Plugins Spark-natifs fonctionnels")
        print("   âœ… SparkDQContext opÃ©rationnel")
        print("   âœ… Cache Spark performant")
        print("   âœ… Conversion Pandas â†’ Spark automatique")
        print("   âœ… Output schema validÃ©")
        print("\nğŸ‰ Votre systÃ¨me DQ est prÃªt pour la production!")
        
    except Exception as e:
        print(f"\nâŒ ERREUR: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
