"""
Spark DQ Context - Context pour ex√©cuter des DQ avec PySpark.

Ce context charge les datasets en Spark DataFrames, permettant des calculs
distribu√©s sur de gros volumes de donn√©es sans charger en m√©moire.

Compatible avec l'architecture BasePlugin existante :
- Les plugins appellent context.load(alias)
- Le context retourne un pyspark.sql.DataFrame
- Les plugins utilisent l'API PySpark pour leurs calculs
"""

from pyspark.sql import SparkSession, DataFrame
from typing import Dict, Optional, Union
import pandas as pd
import logging


class SparkDQContext:
    """
    Context qui retourne TOUJOURS des Spark DataFrames.
    
    Accepte en entr√©e :
    - Fichiers Parquet/CSV
    - Tables Hive/Delta
    - Pandas DataFrames (convertis en Spark automatiquement)
    
    Compatible avec BasePlugin.run(context, **params) o√π les plugins
    attendent context.load() qui retourne un DataFrame.
    
    Exemples:
        # Avec fichiers Parquet
        context = SparkDQContext(catalog={
            "customers": "data/customers.parquet",
            "sales": "data/sales.parquet"
        })
        
        # Avec Pandas DataFrames (pour tests)
        df_test = pd.DataFrame({'col': [1, 2, None]})
        context = SparkDQContext(catalog={
            "test_data": df_test
        })
        
        # Utilisation dans un plugin
        df_spark = context.load("customers")  # pyspark.sql.DataFrame
        result = df_spark.filter(F.col("email").isNull()).count()
    """
    
    def __init__(self, 
                 spark: Optional[SparkSession] = None,
                 catalog: Optional[Dict[str, Union[str, pd.DataFrame]]] = None):
        """
        Initialise le context Spark.
        
        Args:
            spark: Session Spark existante (cr√©√©e automatiquement si None)
            catalog: Mapping alias ‚Üí source
                    - str: chemin fichier (.parquet, .csv) ou nom de table
                    - pd.DataFrame: DataFrame Pandas (converti auto en Spark)
        """
        # Initialiser le logger EN PREMIER (avant de l'utiliser)
        self.logger = logging.getLogger(__name__)
        
        # Puis initialiser les autres attributs
        self.catalog = catalog or {}
        self._cache: Dict[str, DataFrame] = {}
        
        # Cr√©er la session Spark (peut maintenant utiliser self.logger)
        self.spark = spark or self._create_spark_session()
    
    def _create_spark_session(self) -> SparkSession:
        """
        Cr√©e une session Spark locale pour d√©veloppement/test.
        
        Configuration optimis√©e pour dev local :
        - local[*] : utilise tous les cores disponibles
        - shuffle.partitions : r√©duit √† 4 (au lieu de 200 par d√©faut)
        - driver.memory : 2g pour √©viter OOM sur petites machines
        """
        self.logger.info("üöÄ Cr√©ation d'une session Spark locale")
        return SparkSession.builder \
            .appName("DQ_Framework") \
            .master("local[*]") \
            .config("spark.sql.shuffle.partitions", "4") \
            .config("spark.driver.memory", "2g") \
            .getOrCreate()
    
    def load(self, alias: str, cache: bool = True) -> DataFrame:
        """
        Charge un dataset en Spark DataFrame.
        
        Signature compatible avec BasePlugin qui attend context.load(alias).
        
        Args:
            alias: Alias du dataset (ex: "customers")
            cache: Si True, cache le DataFrame Spark pour r√©utilisation
        
        Returns:
            pyspark.sql.DataFrame (distribu√©, pas charg√© en m√©moire)
        
        Raises:
            ValueError: Si l'alias n'existe pas dans le catalogue
        """
        # V√©rifier le cache
        if cache and alias in self._cache:
            self.logger.info(f"üì¶ Dataset '{alias}' charg√© depuis le cache Spark")
            return self._cache[alias]
        
        # V√©rifier le catalogue
        if alias not in self.catalog:
            raise ValueError(
                f"Dataset '{alias}' introuvable dans le catalogue. "
                f"Disponibles: {list(self.catalog.keys())}"
            )
        
        source = self.catalog[alias]
        
        # Cas 1 : Pandas DataFrame ‚Üí Convertir en Spark
        if isinstance(source, pd.DataFrame):
            self.logger.info(f"üîÑ Conversion Pandas ‚Üí Spark pour '{alias}'")
            df = self.spark.createDataFrame(source)
        
        # Cas 2 : Fichier Parquet
        elif isinstance(source, str) and source.endswith('.parquet'):
            self.logger.info(f"üìÇ Chargement Parquet: {source}")
            try:
                df = self.spark.read.parquet(source)
            except Exception:
                # Fallback: parfois le fichier est mal suffix√© (csv) ou corrompu.
                # On tente une lecture CSV pour √™tre tol√©rant en d√©veloppement.
                self.logger.warning(f"‚ö†Ô∏è √âchec lecture Parquet pour '{source}', tentative de lecture en CSV")
                df = self.spark.read.csv(source, header=True, inferSchema=True)
        
        # Cas 3 : Fichier CSV
        elif isinstance(source, str) and source.endswith('.csv'):
            self.logger.info(f"üìÇ Chargement CSV: {source}")
            df = self.spark.read.csv(source, header=True, inferSchema=True)
        
        # Cas 4 : Table Hive/Delta (format: "database.table")
        elif isinstance(source, str):
            self.logger.info(f"üóÑÔ∏è Chargement table: {source}")
            df = self.spark.table(source)
        
        else:
            raise ValueError(
                f"Type de source non support√© pour '{alias}': {type(source)}"
            )
        
        self.logger.info(
            f"‚úÖ Dataset '{alias}' charg√©: {df.count()} lignes, "
            f"{len(df.columns)} colonnes"
        )
        
        # Cacher si demand√©
        if cache:
            df.cache()
            self._cache[alias] = df
        
        return df
    
    def get_columns(self, alias: str) -> list:
        """
        R√©cup√®re la liste des colonnes d'un dataset (M√âTADONN√âES UNIQUEMENT, pas de scan).
        
        Cette m√©thode lit seulement le sch√©ma sans d√©clencher de scan complet des donn√©es.
        Optimis√© pour l'exploration rapide des structures de datasets.
        
        Args:
            alias: Alias du dataset
        
        Returns:
            Liste des noms de colonnes
        """
        return self.peek_schema(alias)
    
    def peek_schema(self, alias: str) -> list:
        """
        Lit le sch√©ma d'un dataset SANS scanner les donn√©es (m√©tadonn√©es seulement).
        
        Optimis√© pour r√©cup√©rer rapidement les colonnes sans overhead de calcul.
        Ne d√©clenche PAS df.count() ou autres actions Spark co√ªteuses.
        
        Args:
            alias: Alias du dataset
        
        Returns:
            Liste des noms de colonnes
            
        Raises:
            ValueError: Si l'alias n'existe pas dans le catalogue
        """
        if alias not in self.catalog:
            raise ValueError(
                f"Dataset '{alias}' introuvable dans le catalogue. "
                f"Disponibles: {list(self.catalog.keys())}"
            )
        
        source = self.catalog[alias]
        
        # Cas 1 : Pandas DataFrame ‚Üí Colonnes directement accessibles
        if isinstance(source, pd.DataFrame):
            self.logger.info(f"üìã Sch√©ma Pandas pour '{alias}': {len(source.columns)} colonnes")
            return source.columns.tolist()
        
        # Cas 2 : Fichier Parquet ‚Üí Lecture sch√©ma seulement
        elif isinstance(source, str) and source.endswith('.parquet'):
            self.logger.info(f"üìã Lecture sch√©ma Parquet: {source}")
            try:
                df = self.spark.read.parquet(source)
            except Exception:
                self.logger.warning(f"‚ö†Ô∏è √âchec lecture Parquet pour '{source}', tentative de lecture en CSV pour le sch√©ma")
                df = self.spark.read.csv(source, header=True, inferSchema=True)
            return df.columns
        
        # Cas 3 : Fichier CSV ‚Üí Lecture sch√©ma seulement
        elif isinstance(source, str) and source.endswith('.csv'):
            self.logger.info(f"üìã Lecture sch√©ma CSV: {source}")
            df = self.spark.read.csv(source, header=True, inferSchema=True)
            return df.columns
        
        # Cas 4 : Table Hive/Delta ‚Üí Lecture sch√©ma depuis metastore
        elif isinstance(source, str):
            self.logger.info(f"üìã Lecture sch√©ma table: {source}")
            df = self.spark.table(source)
            return df.columns
        
        else:
            raise ValueError(
                f"Type de source non support√© pour '{alias}': {type(source)}"
            )
    
    def clear_cache(self):
        """
        Lib√®re le cache Spark pour tous les datasets charg√©s.
        
        Important pour lib√©rer la m√©moire apr√®s l'ex√©cution des DQ.
        """
        if self._cache:
            self.logger.info(f"üßπ Lib√©ration du cache ({len(self._cache)} datasets)")
            for alias, df in self._cache.items():
                df.unpersist()
            self._cache.clear()
    
    def stop(self):
        """
        Arr√™te la session Spark et lib√®re toutes les ressources.
        
        √Ä appeler en fin d'ex√©cution DQ.
        """
        self.clear_cache()
        self.spark.stop()
        self.logger.info("üõë Session Spark arr√™t√©e")
    
    def __enter__(self):
        """Support pour context manager (with statement)"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Nettoyage automatique avec context manager"""
        self.stop()