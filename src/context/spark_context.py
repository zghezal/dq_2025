"""
Spark DQ Context - Context pour exÃ©cuter des DQ avec PySpark.

Ce context charge les datasets en Spark DataFrames, permettant des calculs
distribuÃ©s sur de gros volumes de donnÃ©es sans charger en mÃ©moire.

Compatible avec l'architecture BasePlugin existante :
- Les plugins appellent context.load(alias)
- Le context retourne un pyspark.sql.DataFrame
- Les plugins utilisent l'API PySpark pour leurs calculs
"""

from pyspark.sql import SparkSession, DataFrame
from typing import Dict, Optional, Union
import pandas as pd
import logging


class SparkDQContext:
    """
    Context qui retourne TOUJOURS des Spark DataFrames.
    
    Accepte en entrÃ©e :
    - Fichiers Parquet/CSV
    - Tables Hive/Delta
    - Pandas DataFrames (convertis en Spark automatiquement)
    
    Compatible avec BasePlugin.run(context, **params) oÃ¹ les plugins
    attendent context.load() qui retourne un DataFrame.
    
    Exemples:
        # Avec fichiers Parquet
        context = SparkDQContext(catalog={
            "customers": "data/customers.parquet",
            "sales": "data/sales.parquet"
        })
        
        # Avec Pandas DataFrames (pour tests)
        df_test = pd.DataFrame({'col': [1, 2, None]})
        context = SparkDQContext(catalog={
            "test_data": df_test
        })
        
        # Utilisation dans un plugin
        df_spark = context.load("customers")  # pyspark.sql.DataFrame
        result = df_spark.filter(F.col("email").isNull()).count()
    """
    
    def __init__(self, 
                 spark: Optional[SparkSession] = None,
                 catalog: Optional[Dict[str, Union[str, pd.DataFrame]]] = None):
        """
        Initialise le context Spark.
        
        Args:
            spark: Session Spark existante (crÃ©Ã©e automatiquement si None)
            catalog: Mapping alias â†’ source
                    - str: chemin fichier (.parquet, .csv) ou nom de table
                    - pd.DataFrame: DataFrame Pandas (converti auto en Spark)
        """
        # Initialiser le logger EN PREMIER (avant de l'utiliser)
        self.logger = logging.getLogger(__name__)
        
        # Puis initialiser les autres attributs
        self.catalog = catalog or {}
        self._cache: Dict[str, DataFrame] = {}
        
        # CrÃ©er la session Spark (peut maintenant utiliser self.logger)
        self.spark = spark or self._create_spark_session()
    
    def _create_spark_session(self) -> SparkSession:
        """
        CrÃ©e une session Spark locale pour dÃ©veloppement/test.
        
        Configuration optimisÃ©e pour dev local :
        - local[*] : utilise tous les cores disponibles
        - shuffle.partitions : rÃ©duit Ã  4 (au lieu de 200 par dÃ©faut)
        - driver.memory : 2g pour Ã©viter OOM sur petites machines
        """
        self.logger.info("ğŸš€ CrÃ©ation d'une session Spark locale")
        return SparkSession.builder \
            .appName("DQ_Framework") \
            .master("local[*]") \
            .config("spark.sql.shuffle.partitions", "4") \
            .config("spark.driver.memory", "2g") \
            .getOrCreate()
    
    def load(self, alias: str, cache: bool = True) -> DataFrame:
        """
        Charge un dataset en Spark DataFrame.
        
        Signature compatible avec BasePlugin qui attend context.load(alias).
        
        Args:
            alias: Alias du dataset (ex: "customers")
            cache: Si True, cache le DataFrame Spark pour rÃ©utilisation
        
        Returns:
            pyspark.sql.DataFrame (distribuÃ©, pas chargÃ© en mÃ©moire)
        
        Raises:
            ValueError: Si l'alias n'existe pas dans le catalogue
        """
        # VÃ©rifier le cache
        if cache and alias in self._cache:
            self.logger.info(f"ğŸ“¦ Dataset '{alias}' chargÃ© depuis le cache Spark")
            return self._cache[alias]
        
        # VÃ©rifier le catalogue
        if alias not in self.catalog:
            raise ValueError(
                f"Dataset '{alias}' introuvable dans le catalogue. "
                f"Disponibles: {list(self.catalog.keys())}"
            )
        
        source = self.catalog[alias]
        
        # Cas 1 : Pandas DataFrame â†’ Convertir en Spark
        if isinstance(source, pd.DataFrame):
            self.logger.info(f"ğŸ”„ Conversion Pandas â†’ Spark pour '{alias}'")
            df = self.spark.createDataFrame(source)
        
        # Cas 2 : Fichier Parquet
        elif isinstance(source, str) and source.endswith('.parquet'):
            self.logger.info(f"ğŸ“‚ Chargement Parquet: {source}")
            df = self.spark.read.parquet(source)
        
        # Cas 3 : Fichier CSV
        elif isinstance(source, str) and source.endswith('.csv'):
            self.logger.info(f"ğŸ“‚ Chargement CSV: {source}")
            df = self.spark.read.csv(source, header=True, inferSchema=True)
        
        # Cas 4 : Table Hive/Delta (format: "database.table")
        elif isinstance(source, str):
            self.logger.info(f"ğŸ—„ï¸ Chargement table: {source}")
            df = self.spark.table(source)
        
        else:
            raise ValueError(
                f"Type de source non supportÃ© pour '{alias}': {type(source)}"
            )
        
        self.logger.info(
            f"âœ… Dataset '{alias}' chargÃ©: {df.count()} lignes, "
            f"{len(df.columns)} colonnes"
        )
        
        # Cacher si demandÃ©
        if cache:
            df.cache()
            self._cache[alias] = df
        
        return df
    
    def get_columns(self, alias: str) -> list:
        """
        RÃ©cupÃ¨re la liste des colonnes d'un dataset (sans charger les donnÃ©es).
        
        Args:
            alias: Alias du dataset
        
        Returns:
            Liste des noms de colonnes
        """
        df = self.load(alias)
        return df.columns
    
    def clear_cache(self):
        """
        LibÃ¨re le cache Spark pour tous les datasets chargÃ©s.
        
        Important pour libÃ©rer la mÃ©moire aprÃ¨s l'exÃ©cution des DQ.
        """
        if self._cache:
            self.logger.info(f"ğŸ§¹ LibÃ©ration du cache ({len(self._cache)} datasets)")
            for alias, df in self._cache.items():
                df.unpersist()
            self._cache.clear()
    
    def stop(self):
        """
        ArrÃªte la session Spark et libÃ¨re toutes les ressources.
        
        Ã€ appeler en fin d'exÃ©cution DQ.
        """
        self.clear_cache()
        self.spark.stop()
        self.logger.info("ğŸ›‘ Session Spark arrÃªtÃ©e")
    
    def __enter__(self):
        """Support pour context manager (with statement)"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Nettoyage automatique avec context manager"""
        self.stop()