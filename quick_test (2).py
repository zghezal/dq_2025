#!/usr/bin/env python3
"""
Script de test rapide du syst√®me de plugins.

Ce script peut √™tre ex√©cut√© directement pour v√©rifier que:
- Les plugins sont d√©couverts
- Le catalogue virtuel fonctionne
- Les m√©triques s'ex√©cutent
- La validation des sch√©mas fonctionne

Usage:
    python test_plugin_quick.py
"""

import sys
import pandas as pd
from pathlib import Path

# Ajouter src au path
sys.path.insert(0, str(Path(__file__).parent))

from src.plugins.discovery import discover_all_plugins, REGISTRY
from src.plugins.virtual_catalog import VirtualCatalog


class SimpleContext:
    """Context minimal pour tester."""
    def __init__(self):
        self.data = pd.DataFrame({
            "region": ["North", "South", "North", "East"],
            "amount": [100.0, 200.0, None, 150.0],
        })
    
    def load(self, alias):
        return self.data


class TestRunner:
    """Gestionnaire d'√©tat des tests avec vraie validation."""
    
    def __init__(self):
        self.tests_run = 0
        self.tests_passed = 0
        self.tests_failed = 0
        self.failure_messages = []
    
    def run_test(self, name: str, test_func):
        """
        Ex√©cute un test et enregistre le r√©sultat.
        
        Args:
            name: Nom du test
            test_func: Fonction qui retourne (bool, message)
        """
        self.tests_run += 1
        try:
            success, message = test_func()
            if success:
                self.tests_passed += 1
                print(f"‚úÖ {name}")
            else:
                self.tests_failed += 1
                self.failure_messages.append(f"{name}: {message}")
                print(f"‚ùå {name}")
                print(f"   Raison: {message}")
        except Exception as e:
            self.tests_failed += 1
            self.failure_messages.append(f"{name}: Exception - {str(e)}")
            print(f"‚ùå {name}")
            print(f"   Exception: {e}")
    
    def print_summary(self):
        """Affiche le r√©sum√© final."""
        print("\n" + "=" * 70)
        print("R√âSUM√â DES TESTS")
        print("=" * 70)
        print(f"Tests ex√©cut√©s: {self.tests_run}")
        print(f"Tests r√©ussis:  {self.tests_passed} ‚úÖ")
        print(f"Tests √©chou√©s:  {self.tests_failed} ‚ùå")
        
        if self.tests_failed > 0:
            print("\n‚ö†Ô∏è  √âCHECS D√âTECT√âS:")
            for msg in self.failure_messages:
                print(f"  ‚Ä¢ {msg}")
            print("\n" + "=" * 70)
            print("‚ùå CERTAINS TESTS ONT √âCHOU√â")
            print("=" * 70)
            return False
        else:
            print("\n" + "=" * 70)
            print("üéâ TOUS LES TESTS SONT PASS√âS !")
            print("=" * 70)
            return True


def test_plugin_discovery():
    """Test 1: D√©couverte des plugins."""
    plugins = discover_all_plugins(verbose=False)
    if len(plugins) < 2:
        return False, f"Seulement {len(plugins)} plugin(s) trouv√©(s), attendu >= 2"
    if "missing_rate" not in plugins:
        return False, "missing_rate non d√©couvert"
    if "aggregation_by_column" not in plugins:
        return False, "aggregation_by_column non d√©couvert"
    return True, f"{len(plugins)} plugins d√©couverts"


def test_registry_populated():
    """Test 2: REGISTRY est peupl√©."""
    if len(REGISTRY) == 0:
        return False, "REGISTRY est vide"
    if "missing_rate" not in REGISTRY:
        return False, "missing_rate absent du REGISTRY"
    return True, f"REGISTRY contient {len(REGISTRY)} plugins"


def test_missing_rate_execution():
    """Test 3: Ex√©cution de missing_rate."""
    plugin = REGISTRY["missing_rate"]()
    ctx = SimpleContext()
    result = plugin.run(ctx, dataset="test", column="amount")
    
    if result.value is None:
        return False, "missing_rate n'a pas produit de valeur"
    
    # V√©rifier que la valeur est coh√©rente (1 missing sur 4 = 0.25)
    if abs(result.value - 0.25) > 0.01:
        return False, f"Valeur incorrecte: {result.value}, attendu ~0.25"
    
    return True, f"missing_rate = {result.value:.2%}"


def test_missing_rate_no_output_schema():
    """Test 4: missing_rate ne produit pas de sch√©ma."""
    plugin_class = REGISTRY["missing_rate"]
    schema = plugin_class.output_schema({})
    
    if schema is not None:
        return False, "missing_rate ne devrait pas produire de sch√©ma"
    
    return True, "missing_rate est bien scalaire"


def test_aggregation_has_output_schema():
    """Test 5: aggregation d√©clare son sch√©ma."""
    plugin_class = REGISTRY["aggregation_by_column"]
    params = {
        "dataset": "test",
        "group_by": "region",
        "target": "amount"
    }
    
    schema = plugin_class.output_schema(params)
    
    if schema is None:
        return False, "aggregation devrait produire un sch√©ma"
    
    columns = schema.column_names()
    expected = ["region", "count", "sum_amount", "avg_amount", "min_amount", "max_amount"]
    
    for col in expected:
        if col not in columns:
            return False, f"Colonne manquante: {col}"
    
    return True, f"Sch√©ma avec {len(columns)} colonnes"


def test_aggregation_execution():
    """Test 6: Ex√©cution de aggregation."""
    plugin = REGISTRY["aggregation_by_column"]()
    ctx = SimpleContext()
    params = {
        "dataset": "test",
        "group_by": "region",
        "target": "amount"
    }
    
    result = plugin.run(ctx, **params)
    
    if result.dataframe is None:
        return False, "aggregation n'a pas produit de DataFrame"
    
    df = result.get_dataframe()
    if df is None or len(df) == 0:
        return False, "DataFrame vide"
    
    # V√©rifier les colonnes
    expected_cols = ["region", "count", "sum_amount", "avg_amount", "min_amount", "max_amount"]
    for col in expected_cols:
        if col not in df.columns:
            return False, f"Colonne manquante dans le DataFrame: {col}"
    
    return True, f"DataFrame avec {len(df)} lignes et {len(df.columns)} colonnes"


def test_virtual_catalog_creation():
    """Test 7: Cr√©ation du catalogue virtuel."""
    config = {
        "metrics": [
            {
                "id": "M-001",
                "type": "aggregation_by_column",
                "params": {
                    "dataset": "test",
                    "group_by": "region",
                    "target": "amount"
                }
            },
            {
                "id": "M-002",
                "type": "missing_rate",
                "params": {"dataset": "test", "column": "amount"}
            }
        ]
    }
    
    catalog = VirtualCatalog()
    count = catalog.register_from_config(config)
    
    # Seul M-001 devrait cr√©er un virtual dataset
    if count != 1:
        return False, f"Attendu 1 virtual dataset, obtenu {count}"
    
    if not catalog.exists("virtual:M-001"):
        return False, "virtual:M-001 devrait exister"
    
    if catalog.exists("virtual:M-002"):
        return False, "virtual:M-002 ne devrait PAS exister (m√©trique scalaire)"
    
    return True, f"{count} virtual dataset cr√©√©"


def test_virtual_catalog_columns():
    """Test 8: R√©cup√©ration des colonnes virtuelles."""
    config = {
        "metrics": [
            {
                "id": "M-001",
                "type": "aggregation_by_column",
                "params": {
                    "dataset": "test",
                    "group_by": "region",
                    "target": "amount"
                }
            }
        ]
    }
    
    catalog = VirtualCatalog()
    catalog.register_from_config(config)
    
    try:
        columns = catalog.get_columns("virtual:M-001")
    except Exception as e:
        return False, f"Impossible de r√©cup√©rer les colonnes: {e}"
    
    if "sum_amount" not in columns:
        return False, "Colonne sum_amount manquante"
    
    return True, f"{len(columns)} colonnes r√©cup√©r√©es"


def test_validation_valid_test():
    """Test 9: Validation d'un test valide."""
    config = {
        "metrics": [
            {
                "id": "M-001",
                "type": "aggregation_by_column",
                "params": {
                    "dataset": "test",
                    "group_by": "region",
                    "target": "amount"
                }
            }
        ]
    }
    
    catalog = VirtualCatalog()
    catalog.register_from_config(config)
    
    test_config = {
        "id": "T-001",
        "database": "virtual:M-001",
        "column": "sum_amount"
    }
    
    errors = catalog.validate_test_references(test_config)
    
    if errors:
        return False, f"Test valide rejet√©: {errors}"
    
    return True, "Test valide accept√©"


def test_validation_invalid_dataset():
    """Test 10: Validation rejette dataset invalide."""
    config = {
        "metrics": [
            {
                "id": "M-001",
                "type": "aggregation_by_column",
                "params": {
                    "dataset": "test",
                    "group_by": "region",
                    "target": "amount"
                }
            }
        ]
    }
    
    catalog = VirtualCatalog()
    catalog.register_from_config(config)
    
    test_config = {
        "id": "T-BAD",
        "database": "virtual:M-999",  # N'existe pas
        "column": "foo"
    }
    
    errors = catalog.validate_test_references(test_config)
    
    if not errors:
        return False, "Test invalide devrait √™tre rejet√©"
    
    return True, "Test invalide correctement rejet√©"


def test_validation_invalid_column():
    """Test 11: Validation rejette colonne invalide."""
    config = {
        "metrics": [
            {
                "id": "M-001",
                "type": "aggregation_by_column",
                "params": {
                    "dataset": "test",
                    "group_by": "region",
                    "target": "amount"
                }
            }
        ]
    }
    
    catalog = VirtualCatalog()
    catalog.register_from_config(config)
    
    test_config = {
        "id": "T-BAD",
        "database": "virtual:M-001",
        "column": "colonne_inexistante"  # N'existe pas
    }
    
    errors = catalog.validate_test_references(test_config)
    
    if not errors:
        return False, "Colonne invalide devrait √™tre rejet√©e"
    
    return True, "Colonne invalide correctement rejet√©e"


def test_sequencer_basic():
    """Test 12: S√©quenceur basique."""
    from src.plugins.sequencer import build_execution_plan
    
    config = {
        "metrics": [
            {"id": "M-001", "type": "aggregation_by_column", "params": {}},
            {"id": "M-002", "type": "missing_rate", "params": {}}
        ],
        "tests": [
            {
                "id": "T-001",
                "type": "range",
                "database": "virtual:M-001"
            }
        ]
    }
    
    try:
        plan = build_execution_plan(config)
    except Exception as e:
        return False, f"Erreur de s√©quen√ßage: {e}"
    
    if len(plan.steps) != 3:
        return False, f"Attendu 3 steps, obtenu {len(plan.steps)}"
    
    # T-001 doit d√©pendre de M-001
    t_step = next((s for s in plan.steps if s.id == "T-001"), None)
    if not t_step:
        return False, "T-001 introuvable"
    
    if "M-001" not in t_step.depends_on:
        return False, f"T-001 devrait d√©pendre de M-001, depends_on={t_step.depends_on}"
    
    return True, f"Plan avec {len(plan.steps)} steps, max_level={plan.max_level}"


def test_sequencer_levels():
    """Test 13: Niveaux du s√©quenceur."""
    from src.plugins.sequencer import build_execution_plan
    
    config = {
        "metrics": [
            {"id": "M-001", "type": "aggregation_by_column", "params": {}}
        ],
        "tests": [
            {"id": "T-001", "type": "range", "database": "virtual:M-001"}
        ]
    }
    
    plan = build_execution_plan(config)
    
    # M-001 doit √™tre niveau 0
    m_step = next((s for s in plan.steps if s.id == "M-001"), None)
    if m_step.level != 0:
        return False, f"M-001 devrait √™tre niveau 0, obtenu {m_step.level}"
    
    # T-001 doit √™tre niveau 1
    t_step = next((s for s in plan.steps if s.id == "T-001"), None)
    if t_step.level != 1:
        return False, f"T-001 devrait √™tre niveau 1, obtenu {t_step.level}"
    
    return True, "Niveaux correctement assign√©s"


def main():
    print("=" * 70)
    print("TEST RAPIDE DU SYST√àME DE PLUGINS")
    print("=" * 70)
    
    runner = TestRunner()
    
    # Section 1: D√©couverte
    print("\n[SECTION 1] D√âCOUVERTE DES PLUGINS")
    print("-" * 70)
    discover_all_plugins(verbose=True)  # Afficher les d√©tails de d√©couverte
    
    runner.run_test("1.1 - Plugins d√©couverts", test_plugin_discovery)
    runner.run_test("1.2 - REGISTRY peupl√©", test_registry_populated)
    
    # Section 2: Missing Rate
    print("\n[SECTION 2] PLUGIN MISSING_RATE")
    print("-" * 70)
    runner.run_test("2.1 - Ex√©cution missing_rate", test_missing_rate_execution)
    runner.run_test("2.2 - missing_rate sans output_schema", test_missing_rate_no_output_schema)
    
    # Section 3: Aggregation
    print("\n[SECTION 3] PLUGIN AGGREGATION")
    print("-" * 70)
    runner.run_test("3.1 - output_schema d√©clar√©", test_aggregation_has_output_schema)
    runner.run_test("3.2 - Ex√©cution aggregation", test_aggregation_execution)
    
    # Section 4: Catalogue Virtuel
    print("\n[SECTION 4] CATALOGUE VIRTUEL")
    print("-" * 70)
    runner.run_test("4.1 - Cr√©ation catalogue", test_virtual_catalog_creation)
    runner.run_test("4.2 - R√©cup√©ration colonnes", test_virtual_catalog_columns)
    
    # Section 5: Validation
    print("\n[SECTION 5] VALIDATION DES TESTS")
    print("-" * 70)
    runner.run_test("5.1 - Test valide accept√©", test_validation_valid_test)
    runner.run_test("5.2 - Dataset invalide rejet√©", test_validation_invalid_dataset)
    runner.run_test("5.3 - Colonne invalide rejet√©e", test_validation_invalid_column)
    
    # Section 6: S√©quenceur
    print("\n[SECTION 6] S√âQUENCEUR")
    print("-" * 70)
    runner.run_test("6.1 - S√©quen√ßage basique", test_sequencer_basic)
    runner.run_test("6.2 - Assignation des niveaux", test_sequencer_levels)
    
    # R√©sum√© final
    all_passed = runner.print_summary()
    
    if all_passed:
        print("\nLe syst√®me de plugins fonctionne correctement:")
        print("  ‚úì D√©couverte automatique")
        print("  ‚úì Ex√©cution des m√©triques")
        print("  ‚úì Sch√©mas de sortie")
        print("  ‚úì Catalogue virtuel")
        print("  ‚úì Validation des tests")
        print("  ‚úì S√©quenceur d'ex√©cution")  # üëà NOUVEAU
        print("\nVous pouvez maintenant:")
        print("  1. Tester le s√©quenceur: python demo_sequencer.py")
        print("  2. Lancer les tests complets: pytest tests/test_plugin_system.py -v")
        print("  3. Cr√©er l'executor pour ex√©cuter les plans")
        print("  4. Commencer l'int√©gration avec l'UI")
        return 0
    else:
        print("\n‚ö†Ô∏è  Corrigez les erreurs ci-dessus avant de continuer.")
        return 1


if __name__ == "__main__":
    sys.exit(main())
