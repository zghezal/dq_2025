"""
Export Excel complet - Avec tests implicites de filtres et gestion des d√©pendances

D√©montre:
1. Export avec tous les types de tests implicites
2. Gestion des d√©pendances (SKIPPED si d√©pendance √©choue)
3. Flags d'ex√©cution pour m√©triques et tests
"""

from datetime import datetime
import random
from pathlib import Path

from src.core.dq_parser import DQConfig, DQContext, Metric, Test
from src.core.dq_parser import MetricIdentification, MetricNature, MetricGeneral
from src.core.dq_parser import TestIdentification, TestNature, TestGeneral
from src.core.sequencer import DQSequencer, CommandType
from src.core.dependency_executor import DQExecutor, ExecutionStatus
from src.core.excel_exporter import export_execution_results
import pandas as pd


def create_config_with_filters():
    """Cr√©e une config avec des tests utilisant des filtres"""
    from src.core.dq_parser import load_dq_config
    
    # Charger config de base
    config = load_dq_config("dq/definitions/sales_complete_quality.yaml")
    
    # Ajouter un test avec filtre
    test_with_filter = Test(
        test_id="T_007_check_amounts_north_region",
        type="interval_check",
        identification=TestIdentification(
            test_id="T_007_check_amounts_north_region",
            control_name="Amount validation for North region",
            control_id="CTRL_007"
        ),
        nature=TestNature(
            name="Validation montants r√©gion North",
            description="V√©rifie que les montants de la r√©gion North sont dans la plage valide",
            functional_category_1="Coh√©rence",
            functional_category_2="Donn√©es r√©gionales",
            category="business_rule"
        ),
        general=TestGeneral(
            severity="medium",
            stop_on_failure=False,
            action_on_fail="alert"
        ),
        specific={
            'value_from_dataset': 'sales_2024',
            'target_mode': 'dataset',
            'where': "region = 'North' AND date > '2024-01-01'",
            'bounds': {'lower': 50, 'upper': 300},
            'column_rules': []
        }
    )
    
    config.tests[test_with_filter.test_id] = test_with_filter
    
    return config


def create_metric_dataframe(cmd):
    """
    Cr√©e un DataFrame simul√© pour une m√©trique
    
    Simule le r√©sultat d'une m√©trique missing_rate avec les colonnes
    <column>_missing_rate et <column>_missing_number
    """
    import pandas as pd
    
    # R√©cup√©rer les param√®tres
    specific = cmd.parameters
    columns = specific.get('column', [])
    
    # Si column n'est pas une liste, la convertir
    if isinstance(columns, str):
        columns = [columns]
    elif not columns:
        columns = ['value']  # Colonne par d√©faut
    
    # Cr√©er des donn√©es simul√©es
    data = {}
    for col in columns:
        data[f'{col}_missing_rate'] = [round(random.uniform(0, 0.1), 4)]
        data[f'{col}_missing_number'] = [random.randint(0, 50)]
    
    return pd.DataFrame(data)


def simulate_execution_with_dependencies(sequence):
    """
    Simulation avec gestion r√©elle des d√©pendances.
    Utilise DQExecutor pour g√©rer automatiquement les SKIP.
    """
    
    def execute_command(cmd):
        """Fonction d'ex√©cution simul√©e pour une commande"""
        rand = random.random()
        
        if cmd.command_type == CommandType.METRIC:
            # M√©triques: forcer l'√©chec de M_002 pour tester les d√©pendances
            if cmd.element_id == 'M_002_missing_amount':
                return {
                    'status': ExecutionStatus.ERROR,
                    'value': None,
                    'dataframe': None,
                    'error': 'Connexion timeout to database',
                }
            
            # Autres m√©triques: 90% succ√®s
            if rand > 0.1:
                df_result = create_metric_dataframe(cmd)
                return {
                    'status': ExecutionStatus.SUCCESS,
                    'value': round(random.uniform(0, 0.08), 4),
                    'dataframe': df_result,
                    'error': '',
                }
            else:
                return {
                    'status': ExecutionStatus.ERROR,
                    'value': None,
                    'dataframe': None,
                    'error': f'Dataset {cmd.parameters.get("dataset", "unknown")} not accessible',
                }
        
        elif cmd.command_type == CommandType.TEST:
            # Tests normaux: 80% succ√®s
            if rand > 0.2:
                passed = rand > 0.3
                return {
                    'status': ExecutionStatus.SUCCESS if passed else ExecutionStatus.FAIL,
                    'result': 'PASS' if passed else 'FAIL',
                    'error': '' if passed else f'Value {round(random.uniform(0.1, 0.2), 4)} outside bounds',
                }
            else:
                return {
                    'status': ExecutionStatus.ERROR,
                    'result': 'ERROR',
                    'error': 'Runtime error during test execution',
                }
        
        else:  # Tests implicites
            # Tests techniques : tr√®s haute r√©ussite
            if 'param_validation' in cmd.command_id:
                # Validation param√®tres : 98% succ√®s
                if rand > 0.02:
                    return {
                        'status': ExecutionStatus.SUCCESS,
                        'result': 'PASS',
                        'error': '',
                    }
                else:
                    return {
                        'status': ExecutionStatus.FAIL,
                        'result': 'FAIL',
                        'error': 'Parameter type mismatch: expected int, got string',
                    }
            
            elif 'presence' in cmd.command_id:
                # Pr√©sence colonnes : 95% succ√®s
                if rand > 0.05:
                    return {
                        'status': ExecutionStatus.SUCCESS,
                        'result': 'PASS',
                        'error': '',
                    }
                else:
                    cols = cmd.parameters.get('required_columns', [])
                    missing_col = cols[0] if cols else 'unknown'
                    return {
                        'status': ExecutionStatus.FAIL,
                        'result': 'FAIL',
                        'error': f'Column "{missing_col}" not found in dataset',
                    }
            
            elif 'type' in cmd.command_id:
                # Type des colonnes : 95% succ√®s
                if rand > 0.05:
                    return {
                        'status': ExecutionStatus.SUCCESS,
                        'result': 'PASS',
                        'error': '',
                    }
                else:
                    return {
                        'status': ExecutionStatus.FAIL,
                        'result': 'FAIL',
                        'error': 'Column type mismatch: expected numeric, got string',
                    }
    
    # Utiliser DQExecutor pour g√©rer automatiquement les d√©pendances
    executor = DQExecutor(sequence)
    results = executor.execute(execute_command, skip_on_dependency_failure=True)
    
    return results, executor


def demo_excel_export_complete():
    """D√©monstration compl√®te avec tous les types de tests"""
    
    print("=" * 80)
    print("EXPORT EXCEL COMPLET - AVEC FILTRES ET TESTS IMPLICITES")
    print("=" * 80)
    
    # 1. Cr√©er configuration enrichie
    print("\nüìù Cr√©ation de la configuration avec filtres...")
    config = create_config_with_filters()
    print(f"   M√©triques: {len(config.metrics)}")
    print(f"   Tests: {len(config.tests)}")
    
    # 2. Construire la s√©quence
    print("\nüîÑ Construction de la s√©quence d'ex√©cution...")
    sequencer = DQSequencer(config)
    sequence = sequencer.build_sequence()
    
    # Compter les types de commandes
    metrics_count = sum(1 for c in sequence.commands if c.command_type == CommandType.METRIC)
    tests_count = sum(1 for c in sequence.commands if c.command_type == CommandType.TEST)
    implicit_count = sum(1 for c in sequence.commands if c.command_type == CommandType.IMPLICIT_TEST)
    
    print(f"   Total commandes: {len(sequence.commands)}")
    print(f"   - M√©triques: {metrics_count}")
    print(f"   - Tests: {tests_count}")
    print(f"   - Tests implicites: {implicit_count}")
    
    # 3. Simuler l'ex√©cution avec gestion des d√©pendances
    print("\n‚öôÔ∏è  Simulation de l'ex√©cution avec d√©pendances...")
    execution_results, executor = simulate_execution_with_dependencies(sequence)
    
    # Statistiques avec gestion des SKIPPED
    summary = executor.get_summary()
    metrics_summary = executor.get_metrics_summary()
    tests_summary = executor.get_tests_summary()
    
    print(f"   M√©triques: SUCCESS={metrics_summary.get(ExecutionStatus.SUCCESS, 0)}, "
          f"ERROR={metrics_summary.get(ExecutionStatus.ERROR, 0)}, "
          f"SKIPPED={metrics_summary.get(ExecutionStatus.SKIPPED, 0)}")
    
    print(f"   Tests: PASS={tests_summary.get(ExecutionStatus.SUCCESS, 0)}, "
          f"FAIL={tests_summary.get(ExecutionStatus.FAIL, 0)}, "
          f"ERROR={tests_summary.get(ExecutionStatus.ERROR, 0)}, "
          f"SKIPPED={tests_summary.get(ExecutionStatus.SKIPPED, 0)}")
    
    # 4. G√©n√©rer le rapport Excel
    print("\nüìä G√©n√©ration du rapport Excel complet...")
    output_path = "reports/dq_execution_report_complete.xlsx"
    
    export_execution_results(
        sequence=sequence,
        execution_results=execution_results,
        output_path=output_path,
        quarter="Q4 2025",
        project="Sales Data Quality - Complete",
        run_version="v1.0.1",
        user="data_quality_team"
    )
    
    print("\n" + "=" * 80)
    print("‚ú® Rapport complet g√©n√©r√©!")
    print("=" * 80)
    print(f"\nüìÅ Fichier: {output_path}")
    print("\nüìã Contenu:")
    print(f"  ‚Ä¢ Onglet 'M√©triques': {metrics_count} lignes")
    print(f"  ‚Ä¢ Onglet 'Tests': {tests_count + implicit_count} lignes")
    
    # Lister les onglets de donn√©es export√©s
    print("\nüìä Onglets de donn√©es des m√©triques (export=True):")
    exported_metrics = []
    for cmd in sequence.commands:
        if cmd.command_type == CommandType.METRIC:
            general = cmd.metadata.get('general', {})
            result = execution_results.get(cmd.command_id, {})
            if general.get('export', True) and result.get('status') == 'SUCCESS':
                identification = cmd.metadata.get('identification', {})
                metric_id = identification.get('metric_id', cmd.element_id)
                exported_metrics.append(metric_id)
    
    if exported_metrics:
        for i, metric_id in enumerate(exported_metrics, 1):
            print(f"  {i}. Onglet '{metric_id}' - Donn√©es de la m√©trique")
    else:
        print("  (Aucune m√©trique export√©e)")
    
    print("\nüîç Types de tests inclus:")
    print("  ‚úÖ Tests business (normaux)")
    print("  ‚úÖ Tests techniques de validation de param√®tres")
    print("  ‚úÖ Tests techniques de pr√©sence de colonnes (filtres)")
    print("  ‚úÖ Tests techniques de compatibilit√© de types (filtres)")
    
    print("\n‚ö†Ô∏è  Gestion des d√©pendances:")
    print(f"  ‚Ä¢ Tests/m√©triques SKIPPED: {summary.get(ExecutionStatus.SKIPPED, 0)}")
    
    # Afficher quelques exemples de SKIPPED
    skipped_examples = [(k, v) for k, v in execution_results.items() 
                       if v.get('status') == ExecutionStatus.SKIPPED][:3]
    if skipped_examples:
        print("\n  Exemples d'√©l√©ments SKIPPED:")
        for cmd_id, result in skipped_examples:
            print(f"    - {cmd_id}: {result.get('error', 'N/A')}")


if __name__ == "__main__":
    # Cr√©er le dossier reports
    Path("reports").mkdir(exist_ok=True)
    
    demo_excel_export_complete()
