# coding: utf-8
"""
Test simplifi√© de l'investigation pour interval_check

Utilise le syst√®me simple (dq_runner.py) pour valider le concept
"""

import sys
import io
from pathlib import Path
import pandas as pd

# Force UTF-8
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

repo_root = Path(__file__).parent
sys.path.insert(0, str(repo_root))

from src.dq_runner import run_dq_config

print("=" * 80)
print("TEST SIMPLIFIE - INVESTIGATION AVEC INTERVAL_CHECK SIMUL√â")
print("=" * 80)
print()

# ============================================================================
# 1. CR√âER DES DONN√âES DE TEST
# ============================================================================
print("[1] Cr√©ation des donn√©es")
print("-" * 80)

# Dataset avec probl√®mes de qualit√©
df = pd.DataFrame({
    'customer_id': range(1, 51),
    'email': [f'client{i}@example.com' if i % 5 != 0 else None for i in range(1, 51)],  # 20% missing
    'phone': [f'555-{i:04d}' if i % 7 != 0 else None for i in range(1, 51)],  # ~14% missing
    'age': [20 + i if i % 10 != 0 else -1 for i in range(1, 51)],  # Quelques invalides
    'country': ['FR'] * 30 + ['US'] * 10 + ['UK'] * 10
})

print(f"‚úÖ Dataset cr√©√© : {len(df)} lignes")
print(f"   Email missing : {df['email'].isna().sum()} ({df['email'].isna().sum()/len(df)*100:.1f}%)")
print(f"   Phone missing : {df['phone'].isna().sum()} ({df['phone'].isna().sum()/len(df)*100:.1f}%)")
print(f"   √Çges invalides : {(df['age'] < 0).sum()}")
print()

# ============================================================================
# 2. D√âFINIR UNE CONFIG DQ
# ============================================================================
print("[2] D√©finition de la config DQ")
print("-" * 80)

# Config avec m√©triques + tests
# Simule un interval_check qui v√©rifie que missing_rate < 10%
dq_config = {
    "id": "quality_check",
    "metrics": [
        {
            "id": "email_missing_rate",
            "type": "missing_rate",
            "column": "email"
        },
        {
            "id": "phone_missing_rate",
            "type": "missing_rate",
            "column": "phone"
        },
        {
            "id": "invalid_ages",
            "type": "count_where",
            "filter": "age < 0"
        }
    ],
    "tests": [
        {
            "id": "test_email_quality",
            "type": "range",
            "metric": "email_missing_rate",
            "low": 0,
            "high": 0.10,  # Max 10% autoris√©, mais on a 20% donc KO
            "inclusive": True
        },
        {
            "id": "test_phone_quality",
            "type": "range",
            "metric": "phone_missing_rate",
            "low": 0,
            "high": 0.15,  # Max 15% autoris√©, on a ~14% donc OK
            "inclusive": True
        },
        {
            "id": "test_no_invalid_ages",
            "type": "range",
            "metric": "invalid_ages",
            "low": 0,
            "high": 0,  # Aucun autoris√© donc KO
            "inclusive": True
        }
    ]
}

print("‚úÖ Config DQ d√©finie :")
print(f"   - {len(dq_config['metrics'])} m√©triques")
print(f"   - {len(dq_config['tests'])} tests")
print()

# ============================================================================
# 3. EX√âCUTION SANS INVESTIGATION
# ============================================================================
print("[3] Ex√©cution SANS investigation")
print("-" * 80)

result_no_inv = run_dq_config(df, dq_config, investigate=False)

print("M√©triques :")
for metric_id, metric_result in result_no_inv['metrics'].items():
    value = metric_result.get('value', 'N/A')
    print(f"  - {metric_id} : {value}")

print("\nTests :")
failed = 0
for test_id, test_result in result_no_inv['tests'].items():
    passed = test_result.get('passed', False)
    status = "‚úÖ PASS" if passed else "‚ùå FAIL"
    print(f"  {status} {test_id}")
    if not passed:
        print(f"      ‚Üí {test_result.get('message', '')}")
        failed += 1

print(f"\nR√©sultat : {len(result_no_inv['tests']) - failed}/{len(result_no_inv['tests'])} tests r√©ussis")
print()

# ============================================================================
# 4. EX√âCUTION AVEC INVESTIGATION
# ============================================================================
print("[4] Ex√©cution AVEC investigation")
print("-" * 80)

result_with_inv = run_dq_config(df, dq_config, investigate=True)

print("M√©triques :")
for metric_id, metric_result in result_with_inv['metrics'].items():
    value = metric_result.get('value', 'N/A')
    print(f"  - {metric_id} : {value}")

print("\nTests :")
failed = 0
for test_id, test_result in result_with_inv['tests'].items():
    passed = test_result.get('passed', False)
    status = "‚úÖ PASS" if passed else "‚ùå FAIL"
    print(f"  {status} {test_id}")
    if not passed:
        print(f"      ‚Üí {test_result.get('message', '')}")
        failed += 1

print(f"\nR√©sultat : {len(result_with_inv['tests']) - failed}/{len(result_with_inv['tests'])} tests r√©ussis")
print()

# ============================================================================
# 5. AFFICHER LES INVESTIGATIONS
# ============================================================================
if 'investigations' in result_with_inv and result_with_inv['investigations']:
    print("=" * 80)
    print("üîç INVESTIGATIONS G√âN√âR√âES")
    print("=" * 80)
    print()
    
    investigations = result_with_inv['investigations']
    print(f"Nombre d'investigations : {len(investigations)}")
    print()
    
    for i, inv in enumerate(investigations, 1):
        print(f"{i}. Investigation : {inv['test_id']}")
        print("-" * 80)
        print(f"   Type de m√©trique : {inv.get('metric_type', 'N/A')}")
        print(f"   Valeur m√©trique : {inv.get('metric_value', 'N/A')}")
        
        if 'filter_condition' in inv:
            print(f"   Condition : {inv['filter_condition']}")
            print(f"   Lignes correspondantes : {inv.get('total_matching_rows', 'N/A')}")
        elif 'total_problematic_rows' in inv:
            print(f"   Lignes probl√©matiques : {inv['total_problematic_rows']}")
        
        print(f"   √âchantillon : {inv.get('sample_size', 0)} lignes")
        print(f"   Fichier : {Path(inv.get('sample_file', '')).name}")
        print(f"   Description : {inv.get('description', 'N/A')}")
        
        # Aper√ßu du contenu
        sample_file = Path(inv.get('sample_file', ''))
        if sample_file.exists():
            print(f"\n   Aper√ßu du fichier :")
            sample_df = pd.read_csv(sample_file)
            print(f"   {len(sample_df)} lignes, colonnes : {list(sample_df.columns)}")
            print()
            # Afficher les 3 premi√®res lignes
            for idx, row in sample_df.head(3).iterrows():
                values = [f"{col}={row[col]}" for col in sample_df.columns[:4]]  # Premi√®res colonnes
                print(f"   Ligne {idx+1}: {', '.join(values)}")
            if len(sample_df) > 3:
                print(f"   ... et {len(sample_df) - 3} lignes suppl√©mentaires")
        print()
    
    # Rapport consolid√©
    if 'investigation_report' in result_with_inv:
        print("üìÑ Rapport consolid√© g√©n√©r√© :")
        print(f"   {result_with_inv['investigation_report']}")
        print()
        
        report_path = Path(result_with_inv['investigation_report'])
        if report_path.exists():
            print("Contenu du rapport :")
            print("-" * 80)
            content = report_path.read_text(encoding='utf-8')
            # Afficher les 30 premi√®res lignes
            lines = content.split('\n')
            for line in lines[:30]:
                print(line)
            if len(lines) > 30:
                print(f"... et {len(lines) - 30} lignes suppl√©mentaires")
            print()

else:
    print("‚ö†Ô∏è  Aucune investigation g√©n√©r√©e")
    print()

print("=" * 80)
print("‚úÖ TEST TERMIN√â")
print("=" * 80)
print()
print("üí° Points valid√©s :")
print("   1. M√©triques calcul√©es (missing_rate, count_where) ‚úÖ")
print("   2. Tests ex√©cut√©s (range simule interval_check) ‚úÖ")
print("   3. Tests √©chou√©s d√©tect√©s ‚úÖ")
print("   4. Investigations g√©n√©r√©es automatiquement ‚úÖ")
print("   5. √âchantillons CSV sauvegard√©s ‚úÖ")
print("   6. Rapport consolid√© cr√©√© ‚úÖ")
print()
print("üìå Note : Ce test utilise le syst√®me simple (dq_runner.py)")
print("   L'impl√©mentation dans interval_check (plugins) suit la m√™me logique")
print()
