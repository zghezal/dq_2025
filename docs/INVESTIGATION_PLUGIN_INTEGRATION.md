# Plan d'intÃ©gration de l'investigation dans le systÃ¨me de plugins

## Architecture proposÃ©e

### 1. Extension du `Result` (dans `src/plugins/base.py`)

```python
class Result(BaseModel):
    passed: Optional[bool] = None
    value: Optional[Any] = None
    dataframe: Optional[Any] = None
    message: Optional[str] = None
    meta: Dict[str, Any] = Field(default_factory=dict)
    
    # NOUVEAU : Investigation
    investigation: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Ã‰chantillon de donnÃ©es problÃ©matiques si le test Ã©choue"
    )
```

### 2. Nouvelle mÃ©thode abstraite dans `BasePlugin`

```python
class BasePlugin:
    # ... existing code ...
    
    def investigate(
        self, 
        context, 
        df: pd.DataFrame, 
        params: Dict[str, Any],
        max_samples: int = 100
    ) -> Optional[Dict[str, Any]]:
        """
        GÃ©nÃ¨re un Ã©chantillon de donnÃ©es problÃ©matiques.
        
        Cette mÃ©thode est appelÃ©e automatiquement par l'executor 
        quand un test Ã©choue (passed=False).
        
        Args:
            context: Context d'exÃ©cution
            df: DataFrame source
            params: ParamÃ¨tres du plugin
            max_samples: Nombre max de lignes Ã  Ã©chantillonner
            
        Returns:
            Dict avec:
            - sample_df: pd.DataFrame Ã©chantillon
            - description: str description
            - total_problematic_rows: int nombre total de lignes problÃ©matiques
            - sample_file: Optional[str] chemin du fichier CSV sauvegardÃ©
            
        Note:
            Par dÃ©faut retourne None (pas d'investigation).
            Les plugins peuvent override pour implÃ©menter leur logique.
        """
        return None
```

### 3. Modification de `executor.py`

```python
def execute(plan, loader, investigate: bool = False, investigation_dir: str = "reports/investigations") -> RunResult:
    ctx = Context(plan.alias_map, loader)
    metrics: Dict[str, Result] = {}
    tests: Dict[str, Result] = {}
    investigations: List[Dict[str, Any]] = []
    
    for step in plan.steps:
        if step.kind == "load":
            ctx.load(step.id)
        elif step.kind == "metric":
            plugin = REGISTRY[step.id]()
            res = plugin.run(ctx, **step.params)
            metrics[step.id] = res
            if res.value is not None:
                ctx.metrics_values[step.id] = res.value
        elif step.kind == "test":
            plugin = REGISTRY[step.id]()
            res = plugin.run(ctx, **step.params)
            tests[step.id] = res
            
            # NOUVEAU : Investigation automatique si le test Ã©choue
            if investigate and not res.passed and hasattr(plugin, 'investigate'):
                # RÃ©cupÃ©rer le DataFrame source
                dataset = step.params.get('specific', {}).get('database')
                if dataset and dataset in ctx.datasets:
                    df = ctx.datasets[dataset]
                    inv_result = plugin.investigate(ctx, df, step.params)
                    if inv_result:
                        inv_result['test_id'] = step.id
                        inv_result['test_type'] = plugin.plugin_id
                        investigations.append(inv_result)
                        # Ajouter Ã  res.investigation
                        res.investigation = inv_result
    
    # Ajouter investigations au RunResult
    result = RunResult(
        run_id=_make_run_id(), 
        metrics=metrics, 
        tests=tests,
        artifacts={'investigations': investigations} if investigations else {}
    )
    
    return result
```

### 4. ImplÃ©mentation dans les plugins de test

#### Exemple : `IntervalCheck.investigate()`

```python
@register
class IntervalCheck(BasePlugin):
    plugin_id = "interval_check"
    label = "Interval Check"
    group = "Validation"
    ParamsModel = IntervalCheckParams

    def run(self, context, **params) -> Result:
        # ... existing code ...
        pass
    
    def investigate(
        self, 
        context, 
        df: pd.DataFrame, 
        params: Dict[str, Any],
        max_samples: int = 100
    ) -> Optional[Dict[str, Any]]:
        """
        Ã‰chantillonne les valeurs hors limites pour interval_check.
        
        Deux modes:
        1. metric_value: Trace back to source dataset of the metric
        2. dataset_columns: Direct investigation on columns
        """
        p = self.ParamsModel(**params)
        
        # Mode 1: metric_value - trace to dataset source
        if p.specific.metric_value:
            return self._investigate_metric_value(context, params, max_samples)
        
        # Mode 2: dataset_columns - direct column investigation
        elif p.specific.dataset_columns:
            return self._investigate_dataset_columns(df, params, max_samples)
        
        return None
    
    def _investigate_metric_value(self, context, params, max_samples):
        """Parse metric_id to find dataset source and filter problematic rows."""
        # Implementation details in src/plugins/tests/interval_check.py
        pass
    
    def _investigate_dataset_columns(self, df, params, max_samples):
        """Filter rows with values outside bounds."""
        # Implementation details in src/plugins/tests/interval_check.py
        pass
```

#### Exemple : `MissingRate` (mÃ©trique avec auto-investigation)

```python
@register
class MissingRate(BasePlugin):
    plugin_id = "missing_rate"
    # ... existing code ...
    
    def investigate(
        self, 
        context, 
        df: pd.DataFrame, 
        params: Dict[str, Any],
        max_samples: int = 100
    ) -> Optional[Dict[str, Any]]:
        """
        Ã‰chantillonne les lignes avec valeurs manquantes.
        """
        p = self.ParamsModel(**params)
        
        # RÃ©cupÃ©rer la colonne cible
        col_config = p.specific.column
        if not col_config:
            return None
        
        column = col_config if isinstance(col_config, str) else col_config[0]
        
        if column not in df.columns:
            return None
        
        # Filtrer les lignes avec valeurs manquantes
        missing_mask = df[column].isna()
        problematic_df = df[missing_mask]
        total_problematic = len(problematic_df)
        
        if total_problematic == 0:
            return None
        
        sample_df = problematic_df.head(max_samples)
        
        # Sauvegarder
        from pathlib import Path
        from datetime import datetime
        inv_dir = Path("reports/investigations")
        inv_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{params.get('id', 'metric')}_missing_values_{timestamp}.csv"
        file_path = inv_dir / filename
        sample_df.to_csv(file_path, index=False)
        
        return {
            "sample_df": sample_df,
            "description": f"Lignes avec valeurs manquantes dans '{column}'",
            "total_problematic_rows": total_problematic,
            "sample_size": len(sample_df),
            "sample_file": str(file_path),
            "column": column
        }
```

### 5. UI Dash : Affichage des investigations

Dans les callbacks de l'UI, afficher le lien vers le fichier CSV quand `res.investigation` existe :

```python
# Dans src/callbacks/dq.py ou Ã©quivalent
def render_test_result(test_id: str, result: Result):
    if not result.passed and result.investigation:
        inv = result.investigation
        return html.Div([
            html.H5(f"âŒ {test_id} - FAILED"),
            html.P(result.message),
            html.Div([
                html.H6("ðŸ” Investigation :"),
                html.P(inv['description']),
                html.P(f"Lignes problÃ©matiques : {inv['total_problematic_rows']}"),
                html.P(f"Ã‰chantillon sauvegardÃ© : {inv['sample_size']} lignes"),
                html.A(
                    "ðŸ“¥ TÃ©lÃ©charger CSV", 
                    href=f"/download/{inv['sample_file']}", 
                    className="btn btn-primary"
                )
            ], className="investigation-box")
        ])
```

## Avantages de cette architecture

1. âœ… **SÃ©paration des responsabilitÃ©s** : Chaque plugin sait comment investiguer ses propres Ã©checs
2. âœ… **ExtensibilitÃ©** : Nouveaux plugins peuvent override `investigate()` avec leur logique spÃ©cifique
3. âœ… **Backward compatible** : `investigate()` retourne `None` par dÃ©faut, pas d'impact sur les plugins existants
4. âœ… **Standardisation** : Format uniforme via `Result.investigation`
5. âœ… **UI-ready** : Investigation disponible immÃ©diatement dans l'interface Dash
6. âœ… **Opt-in** : `investigate=True` au niveau de `execute()`, pas de surcharge si non utilisÃ©

## Migration progressive

1. **Phase 1** : Ã‰tendre `Result` et `BasePlugin.investigate()` (base) âœ…
2. **Phase 2** : Modifier `executor.py` pour dÃ©tecter les Ã©checs et appeler `investigate()` âœ…
3. **Phase 3** : ImplÃ©menter `investigate()` dans interval_check (plugin de test autorisÃ©) âœ…
4. **Phase 4** : Ã‰tendre l'UI pour afficher les investigations
5. **Phase 5** : ImplÃ©menter `investigate()` dans missing_rate (mÃ©trique autorisÃ©e)

## Fichiers Ã  modifier

1. `src/plugins/base.py` : Ajouter `investigation` Ã  `Result`, ajouter mÃ©thode `investigate()` âœ…
2. `src/core/executor.py` : Ajouter logique d'investigation aprÃ¨s tests Ã©chouÃ©s âœ…
3. `src/plugins/tests/interval_check.py` : ImplÃ©menter `investigate()` âœ…
4. `src/plugins/metrics/missing_rate.py` : ImplÃ©menter `investigate()` (optionnel)
5. `tools/run_dq.py` : Ajouter `--investigate` flag
6. `src/callbacks/dq.py` : Afficher investigations dans l'UI

## Code rÃ©utilisable

Le module `src/investigation.py` existant peut Ãªtre refactorisÃ© en helpers :

```python
# src/investigation_helpers.py
from pathlib import Path
from datetime import datetime
import pandas as pd

def save_investigation_sample(
    df: pd.DataFrame, 
    test_id: str, 
    suffix: str,
    output_dir: str = "reports/investigations"
) -> Path:
    """Helper pour sauvegarder un Ã©chantillon d'investigation."""
    inv_dir = Path(output_dir)
    inv_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{test_id}_{suffix}_{timestamp}.csv"
    file_path = inv_dir / filename
    df.to_csv(file_path, index=False)
    
    return file_path
```

Puis chaque plugin utilise ces helpers dans son `investigate()`.


---

## IMPORTANT: Plugins autorisï¿½s

**Ce projet utilise uniquement 2 plugins :**

1. **missing_rate** (mï¿½trique) - Calcule le taux de valeurs manquantes
2. **interval_check** (test) - Valide que mï¿½triques/colonnes sont dans des bornes

Tous les autres plugins mentionnï¿½s dans ce document ï¿½ titre d'exemple (range_test, threshold_test, etc.) ont ï¿½tï¿½ supprimï¿½s du projet.

Le module `src/investigation.py` a ï¿½tï¿½ simplifiï¿½ pour ne supporter que missing_rate et count_where.
Le module `src/plugins/investigation_helpers.py` fournit des helpers rï¿½utilisables pour l'implï¿½mentation de `investigate()` dans les plugins autorisï¿½s.

